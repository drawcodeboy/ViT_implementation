{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b735c164-58ba-41f9-b7ab-d09b4969cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92f34c47-d343-46f4-8e04-958fe06fb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, embedding_length, qkv_vec_length):\n",
    "        '''\n",
    "        embedding_length : embedding 하나의 길이 -> W_(qkv)의 row 값이 된다.\n",
    "        qkv_vec_length :  W_(qkv) 행렬의 col 값\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Query Matrix (Weight)\n",
    "        self.W_q = nn.Parameter(torch.randn(embedding_length, qkv_vec_length, requires_grad=True))\n",
    "        '''\n",
    "        self.W_q = torch.Tensor([[1, 0, 1],\n",
    "                                 [1, 0, 0],\n",
    "                                 [0, 0, 1],\n",
    "                                 [0, 1, 1]])\n",
    "        self.W_q.requires_grad = True\n",
    "        '''\n",
    "        \n",
    "        # Key Matrix (Weight)\n",
    "        self.W_k = nn.Parameter(torch.randn(embedding_length, qkv_vec_length, requires_grad=True))\n",
    "        '''\n",
    "        self.W_k = torch.Tensor([[0, 0, 1],\n",
    "                                 [1, 1, 0],\n",
    "                                 [0, 1, 0],\n",
    "                                 [1, 1, 0]])\n",
    "        self.W_k.requires_grad = True\n",
    "        '''\n",
    "        \n",
    "        # Value Matrix (Weight)\n",
    "        self.W_v = nn.Parameter(torch.randn(embedding_length, qkv_vec_length, requires_grad=True))\n",
    "        '''\n",
    "        self.W_v = torch.Tensor([[0, 2, 0],\n",
    "                                 [0, 3, 0],\n",
    "                                 [1, 0, 3],\n",
    "                                 [1, 1, 0]])\n",
    "        self.W_k.requires_grad = True\n",
    "        '''\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "\n",
    "        # print(f'x shape: {x.shape}')\n",
    "        # print(f'W_q shape: {self.W_q.shape}\\nW_k shape: {self.W_k.shape}\\nW_v shape: {self.W_q.shape}')\n",
    "        # print(f'Q shape: {Q.shape}\\nK shape: {K.shape}\\nV shape: {V.shape}')\n",
    "        # print(f'<Q>\\n{Q}\\n<K>\\n{K}\\n<V>\\n{V}\\n')\n",
    "\n",
    "        attn_scores = Q @ torch.transpose(K, -2, -1)\n",
    "        # print(f'<attention scores>\\n{attn_scores}\\n')\n",
    "\n",
    "        attn_scores_softmax = self.softmax(attn_scores / math.sqrt(K.shape[-1]))\n",
    "        # print(f'attention_score: {attn_scores_softmax.shape}')\n",
    "        # print(f'<attention scores softmax>\\n{attn_scores_softmax}\\n')\n",
    "\n",
    "        weighted_values = attn_scores_softmax @ V\n",
    "        # print(f'weighted values shape: {weighted_values.shape}')\n",
    "        # print(f'<Scaled Dot Product Attention Output>\\n{weighted_values}\\n')\n",
    "        \n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24364e0c-28ce-4fcc-bfd8-e131bf276b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 1., 0.],\n",
       "         [0., 2., 0., 2.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.Tensor([[[1, 0, 1, 0],\n",
    "                       [0, 2, 0, 2],\n",
    "                       [1, 1, 1, 1]]])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77f31304-86b2-4ebd-8deb-62b9a9f1a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = ScaledDotProductAttention(30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eb2b419-6d09-44bc-b0aa-b544c7fa55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ScaledDotProductAttention                [1, 197, 20]              1,800\n",
       "├─Softmax: 1-1                           [1, 197, 197]             --\n",
       "==========================================================================================\n",
       "Total params: 1,800\n",
       "Trainable params: 1,800\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(att, (1, 197, 30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
