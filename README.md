# ViT Implementation
* ViT를 구현하여 MNIST에 Epoch 4번, Batch Size 16으로 학습시켰습니다.
* 학습시킨 모델의 추론은 train/vit_mnist_inference.ipynb에서 확인 가능합니다.
* * *
## Reference
* [Attention Mechanism](https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/)
* [Paper: Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
